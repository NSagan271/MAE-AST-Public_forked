{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import os\n",
    "\n",
    "from scipy import signal\n",
    "from scipy import io\n",
    "\n",
    "import torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.mae_ast import MAE_AST\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATE = \"chunk_patch_75_12LaterEncoder.pt\"\n",
    "checkpoint = torch.load(STATE)\n",
    "\n",
    "cfg = checkpoint[\"cfg\"][\"model\"]\n",
    "task_cfg = checkpoint[\"cfg\"][\"task\"]\n",
    "        \n",
    "model = MAE_AST(SimpleNamespace(**checkpoint[\"cfg\"][\"model\"]), SimpleNamespace(**checkpoint[\"cfg\"][\"task\"]))\n",
    "model.load_state_dict(checkpoint[\"model\"], strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wav_to_spectrogram(self, wav):\n",
    "    return torchaudio.compliance.kaldi.fbank(  # Frame shift and length are standard at  10, 25\n",
    "        waveform=wav,\n",
    "        sample_frequency=self.sample_rate,\n",
    "        use_energy=False,\n",
    "        num_mel_bins=self.feature_dim\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_all_seqs(dir='../data', files=None):\n",
    "    if files is None:\n",
    "        files = map(lambda filename: dir + '/' + filename, os.listdir(dir))\n",
    "\n",
    "    seqs = []\n",
    "    for filename in files:\n",
    "        with h5py.File(filename, 'r') as f:\n",
    "            seqs.extend([f[h5py.h5r.get_name(elem, f.id)][:] for elem in f['chirp_sequence_array']])\n",
    "\n",
    "    return seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs = read_all_seqs()\n",
    "X = seqs[200].T[:, 0:4]\n",
    "plt.figure(figsize=(16, 4))\n",
    "plt.title(\"Mic data\")\n",
    "plt.plot(X)\n",
    "plt.legend([\"Mic 1\", \"Mic 2\", \"Mic 3\", \"Mic 4\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup pytorch\n",
    "\n",
    "GPU_NUM = '0'\n",
    "\n",
    "device = torch.device(('cuda:' + GPU_NUM) if (torch.cuda.is_available() and GPU_NUM != '-1') else 'cpu')\n",
    "print(\"Using device: {}\".format(device))\n",
    "if device == 'cuda':\n",
    "    print(\"Device index: {}\".format(torch.cuda.current_device()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = X[:, 1]\n",
    "model = model.to(device)\n",
    "x = x.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = wav_to_spectrogram(wav.unsqueeze(0))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
